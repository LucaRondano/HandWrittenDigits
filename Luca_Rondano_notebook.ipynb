{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip    # to unzip the dataset\n",
    "import cPickle # to load the dataset\n",
    "import random  # for random initialization of weights and biases\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "training_data, validation_data, test_data = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
    "\n",
    "The 'training_data' is returned as a tuple with two entries.\n",
    "The first entry contains the actual training images of hand-written digits, while the second entry contains the digit values corresponding to the training images.\n",
    "In the 'training_data' there are a total of 50,000 images and their correct classification.\n",
    "Each image is represented by its 28 * 28 = 784 pixels, while the classification is just an integer value between 0 and 9.\n",
    "\n",
    "The 'validation_data' and 'test_data' have the same structure, except each contains only 10,000 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following function to modify the format of the second entry of 'training_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a 10-dimensional unit vector with a 1 in the jth position and zeroes elsewhere. This is used to convert a digit into a corresponding desired output from the neural network, i.e., 2 --> (0,0,1,0,0,0,0,0,0,0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply 'vectorized_result' to the labels of the training set. This is better for the network. In general, that is common practice in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = [np.reshape(x, (784, 1)) for x in training_data[0]]\n",
    "training_results = [vectorized_result(y) for y in training_data[1]]\n",
    "training_data = zip(training_inputs, training_results)\n",
    "validation_inputs = [np.reshape(x, (784, 1)) for x in validation_data[0]]\n",
    "validation_data = zip(validation_inputs, validation_data[1])\n",
    "test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
    "test_data = zip(test_inputs, test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a tuple containing '(training_data, validation_data, test_data)'.\n",
    "\n",
    "In particular, 'training_data' is a list containing 50,000 2-tuples (x, y), where x is a 784-dimensional numpy.ndarray containing the input image and y is a 10-dimensional numpy.ndarray with all zeroes and a 1 in the right place.\n",
    "\n",
    "'validation_data' and 'test_data' are lists containing 50,000 2-tuples (x, y), where x is a 784-dimensional numpy.ndarray containing the input image and y is a digit corresponding to the correct digit for x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#show first element in each set\n",
    "print \"training: {0}\".format(training_data[0])\n",
    "print \"validation: {0}\".format(validation_data[0])\n",
    "print \"test: {0}\".format(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIP NEXT LINE IF YOU WANT TO USE ALL THE TRAINING DATA (you probably want to skip it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#halves the training data\n",
    "training_data=training_data[:25000][:25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the activation function (sigmoid) and its derivative used in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup of a generic feed-forward network and the functions necessary to apply stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        \"\"\"The list 'sizes' contains the number of neurons in the respective layers of the network.\n",
    "        In our network we will work with an input layer of size 784 and an output layer of size 10.\n",
    "        \n",
    "        The biases and weights for the network are initialized randomly, using a Gaussian distribution\n",
    "        with mean 0, and variance 1.\"\"\"\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        self.accuracy = [] #list to store the accuracy over the epochs\n",
    "        self.cost_val = [] #list to store the cost over the epochs\n",
    "        \n",
    "        \"\"\"remember old nabla if using momentum\"\"\"\n",
    "        #self.old_nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \n",
    "        \"\"\"Return the output of the network given the input 'a'.\"\"\"\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient descent.\n",
    "        \n",
    "        If 'test_data' is provided then the network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for tracking progress, but slows things\n",
    "        down substantially.\"\"\"\n",
    "        \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "            self.cross_entropy() #used to save the cost over the epochs in a vector when using the cross entropy\n",
    "            #self.quadratic()     #used to save the cost over the epochs in a vector when using the quadratic cost function\n",
    "            \n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \n",
    "        \"\"\"Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch.\n",
    "        The 'mini_batch' is a list of tuples (x, y), and 'eta' is the learning rate.\"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        \"\"\"Use only one of the following options when running the code! Each one corresponds\n",
    "        to a different choice of regularization\"\"\"\n",
    "        \n",
    "        \"\"\"SGD\"\"\"\n",
    "        #self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        \"\"\"SGD + L2 regularization (weight decay) with lambda = 5\"\"\"\n",
    "        self.weights = [(1-eta*5/len(training_data))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        \"\"\"SGD + L1 regularization with lambda = 5\"\"\"\n",
    "        #self.weights = [w-eta*5/len(training_data)*np.sign(w)-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        \"\"\"SGD + momentum with lambda = 0.1\"\"\"\n",
    "        #self.weights = [w + 0.1*old_nw -(eta/len(mini_batch))*nw\n",
    "                       #for w, nw, old_nw in zip(self.weights, nabla_w, self.old_nabla_w)]\n",
    "        #self.old_nabla_w = nabla_w    \"\"\"remember to enable self.old_nabla_w in the initialization!\"\"\"\n",
    "        \n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        \"\"\"Return a tuple '(nabla_b, nabla_w)' representing the gradient of the cost function.\"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        \"\"\"feedforward\"\"\"\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        \"\"\"backward pass\"\"\"\n",
    "        delta = (activations[-1] - y) # * sigmoid_prime(zs[-1])  #Enable the derivative of the sigmoid only if\n",
    "                                                                 #you want to use the quadratic cost function.\n",
    "                                                                 #This term cancels out when using the cross-entropy.\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \n",
    "        \"\"\"Return the number of test inputs for which the neural network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.\"\"\"\n",
    "        \n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        evaluation = sum(int(x == y) for (x, y) in test_results)\n",
    "        \n",
    "        self.accuracy.append(evaluation)\n",
    "        \n",
    "        return evaluation\n",
    "    \n",
    "    def quadratic(self):\n",
    "        \"\"\"Remembers the cost of the quadratic cost function at each epoch.\"\"\"\n",
    "        val_cost = [np.multiply((self.feedforward(x)-vectorized_result(y)),(self.feedforward(x)-vectorized_result(y)))\n",
    "                   for (x, y) in validation_data]\n",
    "        Sum_val = sum([sum(x[0]) for x in val_cost])\n",
    "        \n",
    "        newCost_val = Sum_val/(2*len(validation_data))\n",
    "        self.cost_val.append(newCost_val)\n",
    "\n",
    "    def cross_entropy(self):\n",
    "        \"\"\"Remember the value of the cross entropy at each epoch.\"\"\"\n",
    "        val_cost = [np.multiply(vectorized_result(y), np.log(self.feedforward(x))).transpose()\n",
    "                   for (x, y) in validation_data]\n",
    "        Sum_val = sum([sum(x[0]) for x in val_cost]) \n",
    "                     \n",
    "        newCost_val = -Sum_val/len(validation_data)\n",
    "        self.cost_val.append(newCost_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do some training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we initialize an instance of the class 'Network' with input layer of size 784 (number of pixels in an image), output layer of size 10 (remember: image classification is a 10-dimensional unit vector). There can be any number of hidden layers of any dimension in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 100, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.SGD(training_data, 30, 10, 0.5, validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network with stochastic gradient descent, choosing the number of epochs, the size of the minibatches and the learning rate. You can also choose a test set to print the accuracies after each epoch. If no such set is chosen, then the training happens anyway but the accuracy reached after each epoch is not evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph showing the cost over the epochs for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(range(0, len(net.cost_val)))\n",
    "y=net.cost_val\n",
    "\n",
    "plt.plot(x,y,label = \"eta=0.5\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cost')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a graph showing the evolution of the accuracy over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(range(0, len(net.accuracy)))\n",
    "y=net.accuracy\n",
    "\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train more with a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.SGD(training_data, 30, 10, 0.1, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = [np.argmax(net.feedforward(x)) for (x, y) in test_data]\n",
    "true_labels = [y for (x, y) in test_data]\n",
    "\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "df_cm = pd.DataFrame(conf_matrix, range(10), range(10))\n",
    "plt.figure(figsize=(14,10))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
